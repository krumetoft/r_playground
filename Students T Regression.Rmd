---
title: "Robust Bayesian Students'T Regression"
output: html_notebook
---

Code walk-through of https://solomonkurz.netlify.app/post/2019-02-02-robust-linear-regression-with-student-s-t-distribution/

```{r}
library(tidyverse)

tibble(x = seq(from = -6, to = 6, by = .01)) %>% 
  expand(x, nu = c(1, 2.5, 5, 10, Inf)) %>% 
  mutate(density = dt(x = x, df = nu),
         nu      = factor(nu, levels = c("Inf", "10", "5", "2.5", "1"))) %>% 
  
  ggplot(aes(x = x, y = density, group = nu, color = nu)) +
  geom_line() +
  scale_color_viridis_d(expression(nu),
                        direction = 1, option = "C", end = .85) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(-5, 5)) +
  xlab(NULL) +
  theme(panel.grid = element_blank()) + 
  labs(title = 'Student’s -distribution with a low nu \nwill have notably heavier tails than the conventional Gaussian distribution.')
```

Another way is to compare how probable relatively extreme values are in a Student’s -distribution relative to the Gaussian. For the sake of demonstration, here we’ll compare Gauss with Student’s  with a  of 5. In the plot above, they are clearly different, but not shockingly so. However, that difference is very notable in the tails.

Let’s look more closely with a table. Below, we compare the probability of a given -score or lower within the Gaussian and a  Student’s . In the rightmost column, we compare the probabilities in a ratio.
```{r}
# here we pic our nu
nu <- 5

tibble(z_score               = 0:-5,
       p_Gauss               = pnorm(z_score, mean = 0, sd = 1),
       p_Student_t           = pt(z_score, df = nu),
       `Student/Gauss ratio` = p_Student_t/p_Gauss) %>%
  mutate_if(is.double, round, digits = 5) %>% 
  knitr::kable()
```
Note how low -scores are more probable in this Student’s  than in the Gaussian. This is most apparent in the Student/Gauss ratio column on the right. A consequence of this is that extreme scores are less influential to your solutions when you use a small- Student’s -distribution in place of the Gaussian. That is, the small- Student’s  is more robust than the Gaussian to unusual and otherwise influential observations.

## Simulation

By the two .6s on the off-diagonal positions, we indicated we’d like our two variables to have a correlation of .6.
```{r}
s <- matrix(c(1, .6, 
              .6, 1), 
             nrow = 2, ncol = 2)
print(s)

#Means vector
m <- c(0, 0)

set.seed(3)

d <- MASS::mvrnorm(n = 100, mu = m, Sigma = s) %>%
  as_tibble() %>%
  rename(y = V1, x = V2)

d <-
  d %>%
  arrange(x)

ggplot(data = d, aes(x,y)) +
  geom_point()

```

```{r}
o <- d %>%
  arrange(x)

o[c(1:2), 1] <- c(5, 4.5)

head(o)

ggplot(data = o, aes(x,y)) +
  geom_point()
```

```{r}
ols0 <- lm(data = d, y ~ 1 + x)
ols1 <- lm(data = o, y ~ 1 + x)
```

```{r}
library(broom)

tidy(ols0) %>% mutate_if(is.double, round, digits = 2)
```

```{r}
tidy(ols1) %>% mutate_if(is.double, round, digits = 2)
```
```{r}
# the well-behaved data
p1 <-
  ggplot(data = d, aes(x = x, y = y)) +
  stat_smooth(method = "lm", color = "grey92", fill = "grey67", alpha = 1, fullrange = T) +
  geom_point(size = 1, alpha = 3/4) +
  scale_x_continuous(limits = c(-4, 4)) +
  coord_cartesian(xlim = c(-3, 3), 
                  ylim = c(-3, 5)) +
  labs(title = "No Outliers") +
  theme(panel.grid = element_blank())

# the data with two outliers
p2 <-
  ggplot(data = o, aes(x = x, y = y, color = y > 3)) +
  stat_smooth(method = "lm", color = "grey92", fill = "grey67", alpha = 1, fullrange = T) +
  geom_point(size = 1, alpha = 3/4) +
  scale_color_viridis_d(option = "A", end = 4/7) +
  scale_x_continuous(limits = c(-4, 4)) +
  coord_cartesian(xlim = c(-3, 3), 
                  ylim = c(-3, 5)) +
  labs(title = "Two Outliers") +
  theme(panel.grid = element_blank(),
        legend.position = "none")

# combine the ggplots with patchwork syntax
library(patchwork)

p1 + p2
```

```{r}
aug0 <- augment(ols0)
aug1 <- augment(ols1)

glimpse(aug1)
```

Cook’s  is a measure of the influence of a given observation on the model. To compute , the model is fit once for each  case, after first dropping that case. Then the difference in the model with all observations and the model with all observations but the th observation, as defined by the Euclidean distance between the estimators. Fahrmeir et al (2013, p. 166) suggest that within the OLS framework “as a rule of thumb, observations with  are worthy of attention, and observations with  should always be examined.” Here we plot  against our observation index, , for both models.

```{r}
bind_rows(
  aug0 %>% mutate(i = 1:n()),  # the well-behaved data
  aug1 %>% mutate(i = 1:n())   # the data with two outliers
) %>%
  mutate(fit = rep(c("fit b0", "fit b1"), each = n()/2)) %>%
  ggplot(aes(x = i, y = .cooksd)) +
  geom_hline(yintercept = .5, color = "white") +
  geom_point(alpha = .5) +
  geom_text(data = tibble(i = 46, 
                          .cooksd = .53,
                          fit = "fit b0"),
            label = "Fahrmeir et al said we might worry around here",
            color = "grey50") +
  coord_cartesian(ylim = c(0, .7)) +
  theme(panel.grid = element_blank(),
        axis.title.x = element_text(face = "italic", family = "Times")) +
    facet_wrap(~ fit)
```
```{r}
library(brms)

b0 <- 
  brm(data = d, 
      family = gaussian,
      y ~ 1 + x,
      prior = c(prior(normal(0, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(cauchy(0, 1),  class = sigma)),
      seed = 1)

b1 <- 
  update(b0, 
         newdata = o,
         seed = 1)
```


